"""
KNN(K 近邻算法, 全拼是: K Nearest Neighbors):
    概述:
        大白话解释: 近朱者赤, 近墨者黑.
        专业话述: 找离测试集最近的哪K个样本, 然后投票, 哪个标签值多, 就用它作为 测试集的 最终结果(标签).
    KNN算法 实现思路:
        思路1: 分类思路.      投票, 选最多的.
        思路2: 回归思路.      求均值.
    KNN算法 回归思路 原理如下:
        1. 基于欧氏距离计算 每个训练集 离 测试集的 距离.
            欧式距离: 对应维度差值的平方和, 开平方根.
        2. 按照距离值, 进行升序排列, 找到距离最小的那 K个样本值.
        3. 回归思路: 求K个样本值的 平均值 -> 作为 测试集的 标签.
    KNN算法 代码实现步骤:
        1. 导包.
            确保你已经安装了机器学习的库, 没装就装一下.   pip install scikit-learn
        2. 创建模型(算法)对象.
        3. 准备训练集(x_train, y_train)
        4. 准备测试集(x_test)
        5. 模型训练.
        6. 模型预测, 并打印预测结果.
"""
# 导包
from sklearn.neighbors import KNeighborsRegressor

# 1. 创建模型(算法)对象.
estimator = KNeighborsRegressor(n_neighbors=2)

# 2. 准备训练集(x_train, y_train)
# x_train = [[0, 0, 1], [1, 1, 0], [3, 10, 10], [4, 11, 12]]

# 差值:     (9, 17, 8)    (10, 5, 1)   (20, 6, 2),  (18, 12, 5)
# 平方和:    434             126           440          493
# 开平方根:  20.83           11.22         20.98         22.20
# todo: knn算法回归思路：对开平方后的结果排序找对对应的标签值（y_train），根据我们传入的k求平均值
x_train = [[12, 27, 3], [13, 15, 10], [23, 16, 9], [21, 22, 6]]
y_train = [0.1, 0.2, 0.3, 0.4]

# 3. 准备测试集(x_test)
x_test = [[3, 10, 11]]

# 4. 模型训练.
estimator.fit(x_train, y_train)

# 5. 模型预测, 并打印预测结果.
y_test = estimator.predict(x_test)
# 打印预测结果.
print(f'预测结果为: {y_test}')